{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaba867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:  Counter({'train': 36766, 'val': 4966})\n",
      "Total queries:  41732\n",
      "Query Types:  Counter({'text': 20267, 'YesNo': 7320, 'Others': 5412, 'choose': 4220, 'number': 2118, 'color': 1830, 'shape': 565})\n",
      "Total ids for query not text:  18954\n",
      "Data Split:  Counter({'train': 36766, 'val': 4966})\n",
      "Total queries:  41732\n",
      "Query Types:  Counter({'text': 20267, 'YesNo': 7320, 'Others': 5412, 'choose': 4220, 'number': 2118, 'color': 1830, 'shape': 565})\n",
      "Total ids for query not text:  2511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): ModuleDict(\n",
       "    (txt_src__link1__txt_src): SAGEConv(-1, 2048)\n",
       "    (img_src__link2__img_src): SAGEConv(-1, 2048)\n",
       "    (txt_src__link3__img_src): SAGEConv(-1, 2048)\n",
       "    (img_src__link4__txt_src): SAGEConv(-1, 2048)\n",
       "  )\n",
       "  (conv3): ModuleDict(\n",
       "    (txt_src__link1__txt_src): SAGEConv(1024, 512)\n",
       "    (img_src__link2__img_src): SAGEConv(1024, 512)\n",
       "    (txt_src__link3__img_src): SAGEConv(1024, 512)\n",
       "    (img_src__link4__txt_src): SAGEConv(1024, 512)\n",
       "  )\n",
       "  (conv4): ModuleDict(\n",
       "    (txt_src__link1__txt_src): SAGEConv(512, 256)\n",
       "    (img_src__link2__img_src): SAGEConv(512, 256)\n",
       "    (txt_src__link3__img_src): SAGEConv(512, 256)\n",
       "    (img_src__link4__txt_src): SAGEConv(512, 256)\n",
       "  )\n",
       "  (conv5): ModuleDict(\n",
       "    (txt_src__link1__txt_src): SAGEConv(256, 256)\n",
       "    (img_src__link2__img_src): SAGEConv(256, 256)\n",
       "    (txt_src__link3__img_src): SAGEConv(256, 256)\n",
       "    (img_src__link4__txt_src): SAGEConv(256, 256)\n",
       "  )\n",
       "  (conv6): ModuleDict(\n",
       "    (txt_src__link1__txt_src): SAGEConv(256, 128)\n",
       "    (img_src__link2__img_src): SAGEConv(256, 128)\n",
       "    (txt_src__link3__img_src): SAGEConv(256, 128)\n",
       "    (img_src__link4__txt_src): SAGEConv(256, 128)\n",
       "  )\n",
       "  (lin1): ModuleDict(\n",
       "    (txt_src): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (img_src): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (act1): ModuleDict(\n",
       "    (txt_src): ReLU()\n",
       "    (img_src): ReLU()\n",
       "  )\n",
       "  (lin2): ModuleDict(\n",
       "    (txt_src): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (img_src): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (act2): ModuleDict(\n",
       "    (txt_src): ReLU()\n",
       "    (img_src): ReLU()\n",
       "  )\n",
       "  (lin3): ModuleDict(\n",
       "    (txt_src): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (img_src): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       "  (_softmax): ModuleDict(\n",
       "    (txt_src): Softmax(dim=-1)\n",
       "    (img_src): Softmax(dim=-1)\n",
       "  )\n",
       "  (conv2): ModuleDict(\n",
       "    (txt_src): SAGEConv(2048, 1024)\n",
       "    (img_src): SAGEConv(2048, 1024)\n",
       "  )\n",
       "  (linqci): ModuleDict(\n",
       "    (txt_src): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (img_src): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  )\n",
       "  (blinq): ModuleDict(\n",
       "    (txt_src): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (img_src): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blinc): ModuleDict(\n",
       "    (txt_src): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (img_src): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blini): ModuleDict(\n",
       "    (txt_src): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (img_src): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.core.numeric import outer\n",
    "from scipy.sparse import data\n",
    "import torch\n",
    "from dataset import WebQnaDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from model import ToyNet\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.nn import to_hetero\n",
    "data_root = \"/home/ubuntu/WebQna/nodes-2611\"\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "device = \"cuda\"\n",
    "\n",
    "print_step = 270\n",
    "val_step = 500\n",
    "\n",
    "webqa_dataset_train = WebQnaDataset(data_root)\n",
    "webqa_dataset_val = WebQnaDataset(data_root, val=True)\n",
    "webqa_dataloader_train = DataLoader(webqa_dataset_train, batch_size, shuffle=True)\n",
    "webqa_dataloader_val = DataLoader(webqa_dataset_val, batch_size, shuffle=True)\n",
    "# key = torch.tensor([10])\n",
    "# d = webqa_dataset.get(idx=10)\n",
    "\n",
    "toy_model = ToyNet()\n",
    "\n",
    "#graph_meta = (['txt_src', 'img_src', 'ques'], [('ques','contains','txt_src'), ('ques','contains','img_src')])\n",
    "graph_meta = (['txt_src', 'img_src'], [('txt_src','link1','txt_src'), ('img_src','link2','img_src'), \n",
    "('txt_src','link3','img_src'), ('img_src','link4','txt_src')])\n",
    "toy_model = to_hetero(toy_model, graph_meta)\n",
    "toy_model = toy_model.to(device)\n",
    "\n",
    "# criterion = torch.nn.BCELoss()\n",
    "class_weights = torch.tensor([1,4], dtype=torch.float32).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(class_weights)\n",
    "optimizer = torch.optim.AdamW(toy_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "toy_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7448403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c91ae119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_src': tensor([[0.0111, 0.1069, 0.0788,  ..., 0.6054, 0.7708, 0.7565],\n",
      "        [0.0111, 0.1069, 0.0788,  ..., 0.2897, 0.9691, 0.2911],\n",
      "        [0.0111, 0.1069, 0.0788,  ..., 0.1004, 1.5569, 0.3077],\n",
      "        ...,\n",
      "        [0.1105, 0.0090, 0.0288,  ..., 1.8435, 0.0694, 0.1065],\n",
      "        [0.1105, 0.0090, 0.0288,  ..., 0.7367, 0.0806, 0.4760],\n",
      "        [0.1105, 0.0090, 0.0288,  ..., 1.6636, 0.1517, 0.3201]]), 'txt_src': tensor([[ 0.0111,  0.1069,  0.0788,  ...,  0.3412,  0.1191,  0.1426],\n",
      "        [ 0.0111,  0.1069,  0.0788,  ...,  0.0501, -0.2372, -0.2367],\n",
      "        [ 0.0111,  0.1069,  0.0788,  ...,  0.0146, -0.0506, -0.5682],\n",
      "        ...,\n",
      "        [ 0.1105,  0.0090,  0.0288,  ...,  0.1018,  0.1988, -0.2655],\n",
      "        [ 0.1105,  0.0090,  0.0288,  ...,  0.1042, -0.1728, -0.0055],\n",
      "        [ 0.1105,  0.0090,  0.0288,  ...,  0.4002, -0.0709, -0.1523]])}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-314da973e692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebqa_dataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, datum in enumerate(webqa_dataloader_train):\n",
    "    print(datum.x_dict)\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582ac44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:  Counter({'train': 36766, 'val': 4966})\n",
      "Total queries:  41732\n",
      "Query Types:  Counter({'text': 20267, 'YesNo': 7320, 'Others': 5412, 'choose': 4220, 'number': 2118, 'color': 1830, 'shape': 565})\n",
      "Total ids for query not text:  2511\n"
     ]
    }
   ],
   "source": [
    "webqa_dataset_val = WebQnaDataset(data_root, val=True)\n",
    "#webqa_dataloader_train = DataLoader(webqa_dataset_train, batch_size, shuffle=True)\n",
    "webqa_dataloader_val = DataLoader(webqa_dataset_val, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaa88e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:  Counter({'train': 36766, 'val': 4966})\n",
      "Total queries:  41732\n",
      "Query Types:  Counter({'text': 20267, 'YesNo': 7320, 'Others': 5412, 'choose': 4220, 'number': 2118, 'color': 1830, 'shape': 565})\n",
      "Total ids for query not text:  18954\n"
     ]
    }
   ],
   "source": [
    "webqa_dataset_train = WebQnaDataset(data_root)\n",
    "#webqa_dataloader_train = DataLoader(webqa_dataset_train, batch_size, shuffle=True)\n",
    "webqa_dataloader_train = DataLoader(webqa_dataset_train, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db72970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import Sequential, SAGEConv, Linear, to_hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d97467e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just want to be sure\n"
     ]
    }
   ],
   "source": [
    "# model = Sequential('x, edge_index', [\n",
    "#     (SAGEConv((-1, -1), 64), 'x, edge_index -> x'),\n",
    "#     ReLU(inplace=True),\n",
    "#     (SAGEConv((-1, -1), 64), 'x, edge_index -> x'),\n",
    "#     ReLU(inplace=True),\n",
    "#     (Linear(-1, 2), 'x -> x'),\n",
    "# ])\n",
    "from model import ToyNet\n",
    "\n",
    "toy_model = ToyNet()\n",
    "toy_model = to_hetero(toy_model, graph_meta)\n",
    "toy_model = toy_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "797ca8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.data.batch.HeteroDataBatch'> <class 'tuple'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-7c9107036724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#assert(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, datum in enumerate(webqa_dataloader_train):\n",
    "    #print(datum.x_dict.keys())\n",
    "    datum = datum.to(device)\n",
    "    outp = toy_model(datum.x_dict, datum.edge_index_dict)\n",
    "    print(type(datum), type(outp))\n",
    "    assert(False)\n",
    "    #assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f3f0bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([550])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum.y_dict['img_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eaedfcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([502])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum.y_dict['txt_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "983cdbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([504])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum.y_dict['txt_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0b721707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[0]['txt_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b2d6433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([551, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[0]['img_src'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "585823fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(outp[0]['img_src'], datum.y_dict['img_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7141d0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f84929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_softmax = torch.nn.Softmax(dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d654bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = _softmax(outp[0]['img_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aafcfbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([551, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b304627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = torch.argmax(ff, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96d6e1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([551])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cd9c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fff, torch.argmax(outp[0]['img_src'], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e201cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([551, 3584])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum.x_dict['img_src'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ff49316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0, 1, 2, 3, 4]\n",
    "node_idx = [i for i in range(len(y))]\n",
    "node_idx = list(np.random.permutation(node_idx))\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "source_nodes = node_idx[:-1]\n",
    "target_nodes = node_idx[1:]\n",
    "edge_index = torch.tensor([source_nodes + target_nodes, target_nodes + source_nodes], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46429273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 4, 0]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5225fb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 0, 1]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf4479b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2, 4, 0, 2, 4, 0, 1],\n",
       "        [2, 4, 0, 1, 3, 2, 4, 0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7220dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
